{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as urllib2\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "Schwab_BASE = 'https://www.schwabjobs.com'\n",
    "PNC_BASE = 'https://www.indeed.com/viewjob?'\n",
    "\n",
    "PNC_URL = \"https://www.indeed.com/jobs?q=Pnc+Bank\"\n",
    "Schwab_URL = \"https://www.schwabjobs.com/List/Custom/Schwab-Job-Category\"\n",
    "\n",
    "stateList = []\n",
    "\n",
    "count = 1\n",
    "BASE = 'https://www.indeed.com/'\n",
    "FIRST_LISTING_PAGE = 'https://www.indeed.com/cmp/Pnc-Financial-Services-Group/jobs?clearPrefilter=1'\n",
    "LISTING_URL_PREFIX = 'https://www.indeed.com/cmp/Pnc-Financial-Services-Group/jobs?start='\n",
    "LISTING_URL_POSTFIX = 'clearPrefilter=1#cmp-menu-container'\n",
    "\n",
    "def keywordsFileName(listId):\n",
    "    if listId == 1:\n",
    "        return 'frequency.csv'\n",
    "    elif listId == 2:\n",
    "        return 'tfIdfWords.csv'\n",
    "    else:\n",
    "        return 'tfIdfWords.csv'\n",
    "\n",
    "def scrape(postingUrl,bank):\n",
    "    print(postingUrl)\n",
    "    global count\n",
    "    headers = [\"Job No\", \"Institution\", \"List Id {1,2,3}\", \"URL (URL of the job posting)\"]\n",
    "\n",
    "    values = [count, \"PNC Bank\", \"2\", postingUrl]\n",
    "    \n",
    "    indices = list(range(1, 101))\n",
    "    \n",
    "    wordCountDict = dict.fromkeys(indices,0)\n",
    "    postingDict = dict(zip(headers, values))\n",
    "    \n",
    "    enc = 'utf-8'\n",
    "    with open(\"Data/frequency_final.csv\", 'r', encoding = enc) as f:\n",
    "        reader = csv.reader(f)\n",
    "        keywords = list(reader)\n",
    "    arr = [i[0] for i in keywords]\n",
    "    \n",
    "    try:\n",
    "        hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "        request = urllib2.Request(postingUrl,headers=hdr)\n",
    "        html_page = urlopen(request)\n",
    "        soup2 =  BeautifulSoup(html_page,\"html.parser\")\n",
    "        if bank == 'PNC':\n",
    "            value = soup2.find('input', {'id':'preLoadJSON'})\n",
    "            for word in value.get('value').split('\"CandLandPageText\"')[1].replace(\"\\\\u003\", \" \").split():\n",
    "                if(word in arr):\n",
    "                    ind = arr.index(word)\n",
    "                    wordCountDict[ind] = wordCountDict.get(ind, 0) + 1\n",
    "            dataDict = postingDict.copy()  \n",
    "            return {**postingDict, **wordCountDict}\n",
    "                \n",
    "        else:\n",
    "            value = soup2.findAll(\"div\", {\"class\": \"desc\"})\n",
    "            for val in value:\n",
    "                st = val.text.split()\n",
    "                for word in st:\n",
    "                    if(word in arr):\n",
    "                        ind = arr.index(word)\n",
    "                        wordCountDict[ind] = wordCountDict.get(ind, 0) + 1\n",
    "    except:\n",
    "        print(\"Exception\")\n",
    "        return (\"An exception occurred\")\n",
    "    \n",
    "    count = count + 1\n",
    "\n",
    "    dataDict = postingDict.copy()  \n",
    "    return {**postingDict, **wordCountDict} \n",
    " \n",
    "def scrape_page_pnc(page):\n",
    "    response = urlopen(page)\n",
    "    bs_obj = BeautifulSoup(response, \"html.parser\")\n",
    "    if bs_obj:\n",
    "        links = bs_obj.findAll('div',{'class': 'cmp-section cmp-with-border'})\n",
    "        for page in links:\n",
    "            linkTags = page.find_all('a', attrs={'href': re.compile(\"/cmp/_\")})\n",
    "            for tag in linkTags:\n",
    "                link = tag.get('href')\n",
    "                postingUrl = BASE + link\n",
    "                pageDict = scrape(postingUrl,\"PNC\")\n",
    "                if (type(pageDict) is dict):\n",
    "                        data = pd.DataFrame([pageDict])\n",
    "                        # if file does not exist write header \n",
    "                        if not os.path.isfile('pnc_freq.csv'):\n",
    "                            data.to_csv('pnc_freq.csv', header='column_names')\n",
    "                        else: \n",
    "                            # else it exists so append without writing the header\n",
    "                            data.to_csv('pnc_freq.csv', mode='a', header=False)\n",
    "                else:\n",
    "                    print(\"exception?\")\n",
    "    \n",
    "\n",
    "\n",
    "def scrape_pages_schwab(pages):\n",
    "    stateList = {}\n",
    "    for p in pages:\n",
    "        pageHtml = urlopen(p)\n",
    "        soup = BeautifulSoup(pageHtml)\n",
    "        table = soup.find('table', 'JobListTable')\n",
    "        pageDataframe = pd.DataFrame([])\n",
    "        for link in table.findAll('a', attrs={'href': re.compile(\"/ShowJob\")}):\n",
    "            page = Schwab_BASE + link.get('href')\n",
    "            state = table.findAll(\"td\",{\"class\": \"colstate\"})\n",
    "            for s in state:\n",
    "                st = re.sub(r'[^ \\w\\.]', '', s.text)\n",
    "                if st not in stateList:\n",
    "                    stateList[st] = 1\n",
    "                else:\n",
    "                    stateList[st] += 1\n",
    "            pageDict = scrape(page,\"Schwab\")\n",
    "            if (type(pageDict) is dict):\n",
    "                data = pd.DataFrame([pageDict])\n",
    "                # if file does not exist write header \n",
    "                if not os.path.isfile('schwabData_tr.csv'):\n",
    "                    print(\"writing to csv\")\n",
    "                    data.to_csv('schwabData_tr.csv', header='column_names')\n",
    "                else: # else it exists so append without writing the header\n",
    "                    data.to_csv('schwabData_tr.csv', mode='a', header=False)\n",
    "    with open('states.csv', 'a+', encoding = 'utf-8') as f: \n",
    "        for key in stateList.keys():\n",
    "            f.write(\"%s,%d\\n\"%(key,stateList[key]))\n",
    "        \n",
    "\n",
    "def get_pages(categories):\n",
    "    pageLinks = []\n",
    "    for key, value in categories.items():\n",
    "        if(\"---\" not in key):\n",
    "            if(value > 30):\n",
    "                pageCount = 1\n",
    "                for i in range(1,value,29):\n",
    "                    pageLinks.append(Schwab_BASE + key + \"Page-\" + str(pageCount))\n",
    "                    pageCount = pageCount + 1\n",
    "                \n",
    "            else:\n",
    "                pageLinks.append(Schwab_BASE + key)\n",
    "    return pageLinks\n",
    "\n",
    "def get_category(response):\n",
    "    bs_obj = BeautifulSoup(response, \"html.parser\")\n",
    "    if bs_obj:\n",
    "        cat_table = bs_obj.find_all(\"table\",{\"class\": \"list-categores-table\"})\n",
    "\n",
    "        catLinks = []\n",
    "        numPages = []\n",
    "        cat = []\n",
    "\n",
    "        for page in cat_table:\n",
    "            linkTags = page.find_all('a', attrs={'href': re.compile(\"/ListJobs\")})\n",
    "            divTags = page.find_all('div')\n",
    "            for tag in linkTags:\n",
    "                catLinks.append(tag.get('href'))\n",
    "                cat.append(tag.text)\n",
    "            for div in divTags:\n",
    "                num = int(re.search(r'\\d+', div.text).group())\n",
    "                numPages.append(num)\n",
    "        #categCsv = dict(zip(cat, numPages))\n",
    "        #plt.bar(range(len(categCsv)), list(categCsv.values()), align='center')\n",
    "        #plt.xticks(range(len(categCsv)), list(categCsv.keys()))\n",
    "        #plt.xticks(rotation = 90)\n",
    "        #plt.imshow()\n",
    "        return dict(zip(catLinks, numPages))\n",
    "\n",
    "def scrape_site(Url):\n",
    "    scrape_page_pnc(FIRST_LISTING_PAGE)\n",
    "    for i in range(12,996,12):\n",
    "        nextPageUrl = LISTING_URL_PREFIX +  str(i) + \"&\" + LISTING_URL_POSTFIX\n",
    "        scrape_page_pnc(nextPageUrl)\n",
    "\n",
    "def get_data(bank,Url):\n",
    "    if (bank == \"PNC\"):\n",
    "        scrape_site(Url)\n",
    "    elif (bank == \"Schwab\"):\n",
    "        res = urlopen(Url)\n",
    "        categories = get_category(res)\n",
    "        pages = get_pages(categories)\n",
    "        scrape_pages_schwab(pages)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_data(\"PNC\",\"https://www.indeed.com/jobs?q=Pnc+Bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
